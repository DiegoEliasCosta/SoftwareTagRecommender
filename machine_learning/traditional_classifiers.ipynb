{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cYFmhhfqkreu"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import gensim\n",
    "import sklearn\n",
    "import sys\n",
    "import scipy\n",
    "import time\n",
    "import warnings\n",
    "import threading\n",
    "from functools import partial\n",
    "import datetime\n",
    "import pickle\n",
    "from sklearn import utils\n",
    "import operator\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from mlxtend.preprocessing import DenseTransformer\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ijBNtUhjkrez"
   },
   "outputs": [],
   "source": [
    "#columns of the data\n",
    "topics_col = 'github_topics_top'\n",
    "text_col = 'input_text_freq'\n",
    "\n",
    "#doc2vec params\n",
    "minCount = 10\n",
    "d2v_max_feat = 1000\n",
    "\n",
    "#tfidf params\n",
    "ngramRange = (1, 2)\n",
    "tfidf_max_feat = 20000\n",
    "\n",
    "#different setting for tf-idf, doc2vc, and tuning modes\n",
    "featuresMode = [\"\", \"doc2vec\"]\n",
    "tuningModes = [\"\", \"grid search\", \"randomized search\"]\n",
    "\n",
    "models_path = 'models/'\n",
    "\n",
    "print('Loading data..')\n",
    "train_df = pd.read_csv('../data/repos_multihot_train.csv')\n",
    "test_df = pd.read_csv('../data/repos_multihot_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4l-Dtb82kre4",
    "outputId": "f2e67c2d-435c-4523-d9f5-01a4a9cba007",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_col = 'text'\n",
    "train_df = train_df.drop(columns=['labels'])\n",
    "test_df = test_df.drop(columns=['labels'])\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6qihObk2krfB",
    "outputId": "295a826c-6afc-458f-d94a-7a71946ac1bc"
   },
   "outputs": [],
   "source": [
    "X_train = train_df[[text_col]]\n",
    "y_train = train_df[train_df.columns.difference([text_col])]\n",
    "\n",
    "X_test = test_df[[text_col]]\n",
    "y_test = test_df[test_df.columns.difference([text_col])]\n",
    "\n",
    "X_train[text_col] = X_train[text_col].astype(str)\n",
    "X_test[text_col] = X_test[text_col].astype(str)\n",
    "\n",
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QQ8m7x7zkrfJ",
    "outputId": "76775280-3891-468d-d29f-25e71f64f6c5"
   },
   "outputs": [],
   "source": [
    "#tuning parameters\n",
    "print('Setting params...')\n",
    "svc_random_grid = {'C': scipy.stats.expon(scale=100), 'gamma': scipy.stats.expon(scale=.1),\n",
    "                   'kernel': ['rbf'], 'class_weight':['balanced', None]}\n",
    "svc_param_grid = {'C': [10, 20, 30, 40], 'gamma': [0, 0.001, 0.008, 0.01, 0.1, 0.5]}\n",
    "###############################################################################\n",
    "sgd_random_grid = {\"loss\": [\"log\"],\n",
    "                   \"alpha\": [0.0001, 0.00001],\n",
    "                   \"penalty\": [\"elasticnet\"],\n",
    "                   \"l1_ratio\": 0.2*np.arange(0,5),\n",
    "                   \"shuffle\": [True],\n",
    "                   \"learning_rate\": ['optimal']}\n",
    "sgd_param_grid = []\n",
    "###############################################################################\n",
    "lr_random_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "lr_param_grid = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_L46iv-GkrfP"
   },
   "outputs": [],
   "source": [
    "class_weights = 'balanced'\n",
    "classifiers = {\n",
    "    \"nb\" : {\"name\" : \"multinomial naive bayes\", \"clf\" : MultinomialNB()},\n",
    "    \"gnb\" : {\"name\" : \"gaussian naive bayes\", \"clf\" : GaussianNB()},\n",
    "    \"lr\" : {\"name\" : \"logistic regression\", \"clf\" : LogisticRegression(n_jobs=-1, class_weight=class_weights),\n",
    "            \"param_grid\" : lr_param_grid, \"random_grid\" : lr_random_grid},\n",
    "    \"sgd\" : {\"name\" : \"stochastic gradient descent \", \n",
    "             \"clf\" : SGDClassifier(n_jobs=-1,class_weight=class_weights, loss='log'), \"param_grid\" : sgd_param_grid, \"random_grid\" : sgd_random_grid},\n",
    "    \"svm\" : {\"name\" : \"support vector machine \", \"clf\" : SVC(probability=True,class_weight=class_weights), \n",
    "             \"param_grid\" : svc_param_grid, \"random_grid\" : svc_random_grid}\n",
    "}\n",
    "methods={\"OneVsRestClassifier\":OneVsRestClassifier,\"ClassifierChain\":ClassifierChain}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rq0ofCQjkrfS"
   },
   "outputs": [],
   "source": [
    "#functions\n",
    "def label_sentences(corpus, label_type):\n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "    a dummy index of the post.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(doc2vec.TaggedDocument(str(v).split(), [label]))\n",
    "    return labeled\n",
    "\n",
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained doc2vec model\n",
    "    :param doc2vec_model: Trained Doc2Vec model\n",
    "    :param corpus_size: Size of the data\n",
    "    :param vectors_size: Size of the embedding vectors\n",
    "    :param vectors_type: Training or Testing vectors\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors\n",
    "\n",
    "def get_vectors_w2v(model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained word2vec model\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.wordvecs[prefix]\n",
    "    return vectors\n",
    "\n",
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, t) for t in text_list ])\n",
    "\n",
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZgZL009IkrfU"
   },
   "outputs": [],
   "source": [
    "def write_in_file(algorithmName, result):\n",
    "    f.write(algorithmName + \":\\n\")\n",
    "    f.write(result)\n",
    "    f.write('-----------------------------------------------------------------')\n",
    "    f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pm3RWFCVkrfW"
   },
   "outputs": [],
   "source": [
    "# Classifiers\n",
    "def classify_multilabel(algorithm, featureMode, tuningMode,method_name=\"OneVsRestClassifier\"):\n",
    "    algorithmName = classifiers[algorithm][\"name\"]\n",
    "    \n",
    "    def update_dict(d):\n",
    "        return {f'estimator__{k}':v for k,v in d.items()}\n",
    "    \n",
    "#     model = \"this will be our model!\"    \n",
    "    if(tuningMode == 0):\n",
    "        print('Tuning' , tuningMode, '-running default settings...')\n",
    "        model = methods[method_name](classifiers[algorithm][\"clf\"])\n",
    "    elif(tuningMode == 1):\n",
    "        print('Tuning' , tuningMode, '-running grid search...')\n",
    "        model = GridSearchCV(estimator = methods[method_name](classifiers[algorithm][\"clf\"]), \n",
    "                             param_grid = update_dict(classifiers[algorithm][\"param_grid\"]), \n",
    "                             cv = 3, n_jobs = -1, verbose = 2)\n",
    "    elif(tuningMode == 2):\n",
    "        print('Tuning' , tuningMode, '-running default randomized search...')\n",
    "        model = RandomizedSearchCV(estimator = methods[method_name](classifiers[algorithm][\"clf\"]), \n",
    "                                   param_distributions = update_dict(classifiers[algorithm][\"random_grid\"]),\n",
    "                                   n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "    \n",
    "    report = \"\"\n",
    "    y_pred_probab = []\n",
    "    if(featureMode == 0):\n",
    "        print('Feature', featureMode, '-running TF-IDF')\n",
    "        algorithmName += \" + ngram range \" + str(ngramRange)\n",
    "        model.fit(tfidf_x_train, y_train)\n",
    "        y_pred_probab = model.predict_proba(tfidf_x_test)        \n",
    "    elif(featureMode == 1):\n",
    "        print('Feature', featureMode, '-running D2V')\n",
    "        algorithmName += \" + min count \" + str(minCount) + \" + features number \" + str(d2v_max_feat)\n",
    "        model.fit(d2v_x_train, y_train)\n",
    "        y_pred_probab = model.predict_proba(d2v_x_test)\n",
    "        \n",
    "    if(tuningMode):\n",
    "        report = \"\\nbestparameters:\\n\" + str(model.best_params_) + '\\n'\n",
    "    \n",
    "    print(report)    \n",
    "    with open(f'{models_path}/{algorithm}--{featureMode}--{tuningMode}--Multilabel.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    return (algorithmName, model, y_pred_probab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NSIpHwArkrfY"
   },
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l9OBGlOJkrfZ"
   },
   "outputs": [],
   "source": [
    "#tfidf\n",
    "print('started tf-idf...')\n",
    "tfidf_vectorizer_title = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{2,}', \n",
    "    ngram_range=ngramRange,\n",
    "    max_features=tfidf_max_feat)\n",
    "\n",
    "tfidf_x_train = tfidf_vectorizer_title.fit_transform(X_train[text_col].values.astype('U'))\n",
    "tfidf_x_test = tfidf_vectorizer_title.transform(X_test[text_col].values.astype('U'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dwn9EKiykrfr"
   },
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_pAIvgIxkrfr",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#doc2vec\n",
    "print('started doc2vec...')\n",
    "X_train_title = label_sentences(X_train[text_col], 'Train')\n",
    "X_test_title = label_sentences(X_test[text_col], 'Test')\n",
    "\n",
    "\n",
    "all_data_title = X_train_title + X_test_title\n",
    "\n",
    "model_dbow = Doc2Vec(dm=0, vector_size=d2v_max_feat, negative=5, min_count=minCount, \n",
    "                           alpha=0.065, min_alpha=0.065, workers = 40)\n",
    "model_dbow.build_vocab([x for x in tqdm(all_data_title)])\n",
    "\n",
    "for epoch in range(30):\n",
    "    print(epoch)\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data_title)]), \n",
    "                           total_examples=len(all_data_title), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha\n",
    "\n",
    "d2v_x_train = get_vectors(model_dbow, len(X_train_title), d2v_max_feat, 'Train')\n",
    "d2v_x_test = get_vectors(model_dbow, len(X_test_title), d2v_max_feat, 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pLQ77am5krfz"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "415npbB9krfz"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def eval(y_original,y_pred, y_pred_probab):\n",
    "    org_label_count_vec = np.sum(y_original, axis=1)\n",
    "    repo_5_tags = len(np.where(org_label_count_vec >= 5)[0])    \n",
    "    def calc(p1, p2, p3, func, **kwargs):\n",
    "        return func(p1, p2, **kwargs)\n",
    "    \n",
    "    \n",
    "    def calc_prob(p1, p2, p3, func, **kwargs):\n",
    "        return func(p1, p3, **kwargs)\n",
    "    \n",
    "    \n",
    "    def success_rate(y_original, y_pred):\n",
    "        common = 0\n",
    "        for i in range(0, y_pred.shape[0]):\n",
    "            if(sum(y_original.values[i] * y_pred[i])) > 0:\n",
    "                common = common +1\n",
    "        success = common/y_pred.shape[0]\n",
    "        return success\n",
    "    \n",
    "    def coverage(y_original,y_pred):\n",
    "        x =  y_pred.sum(axis = 0)\n",
    "        c = np.count_nonzero(x > 0)\n",
    "        cov = c / y_original.shape[1]\n",
    "        return cov\n",
    "    \n",
    "    def prf_at_k(y_original, y_pred_probab):\n",
    "        k_list = [1, 2, 3, 5, 8, 10]\n",
    "        s1,s5 = {}, {}\n",
    "        r, p,f =  {}, {}, {}\n",
    "        y_org_array = y_original.values\n",
    "\n",
    "        for k in k_list:\n",
    "            org_label_count = np.sum(y_org_array, axis=1).tolist()\n",
    "            top_ind = []\n",
    "            top_ind =  np.argpartition(y_pred_probab, -1 * k, axis=1)[:, -1 * k:]\n",
    "            pred_in_org = y_org_array[np.arange(y_org_array.shape[0])[:, None], top_ind]\n",
    "            common_topk = np.sum(pred_in_org, axis=1)\n",
    "            recall, precision, f1 = [], [], []\n",
    "            success1, success5 = 0, 0\n",
    "            for index, value in enumerate(common_topk):    \n",
    "                recall.append(value/min(k, org_label_count[index]))\n",
    "                precision.append(value/k)\n",
    "                if (value >= 1): success1 += 1           \n",
    "                if (value >= 5): success5 += 1         \n",
    "            s1.update({'S1@'+str(k): \"{:.2f}\".format((success1/len(y_original))*100)})\n",
    "            s5.update({'S5@'+str(k): \"{:.2f}\".format((success5/repo_5_tags)*100)})\n",
    "            r.update({'R@'+str(k): \"{:.2f}\".format(np.mean(recall)*100)})           \n",
    "            p.update({'P@'+str(k): \"{:.2f}\".format(np.mean(precision)*100)})\n",
    "            f1 = stats.hmean([precision, recall])\n",
    "            f.update({'F1@'+str(k): \"{:.2f}\".format(np.mean(f1)*100)})\n",
    "        return r, p, f, s1, s2, s3, s4, s5\n",
    "    \n",
    "    r, p ,f, s1, s5  = {},{},{},{},{}\n",
    "    r, p, f, s1, s2, s3, s4, s5 = prf_at_k(y_original, y_pred_probab)\n",
    "\n",
    "    \n",
    "    metrics = {\n",
    "        \"Success_Rate\": partial(calc,func=success_rate),\n",
    "        \"Coverage\": partial(calc,func=coverage),\n",
    "        \"LRL\": partial(calc,func=sklearn.metrics.label_ranking_loss),\n",
    "        \"F1_micro\": partial(calc,func=sklearn.metrics.f1_score,average='micro'),\n",
    "        \"F1_macro\": partial(calc,func=sklearn.metrics.f1_score,average='macro'),\n",
    "        \"F1_weighted\": partial(calc,func=sklearn.metrics.f1_score,average='weighted'),\n",
    "        \"F1_samples\": partial(calc,func=sklearn.metrics.f1_score,average='samples'),\n",
    "        \"P_micro\": partial(calc,func=sklearn.metrics.precision_score,average='micro'),\n",
    "        \"P_macro\": partial(calc,func=sklearn.metrics.precision_score,average='macro'),\n",
    "        \"P_weighted\": partial(calc,func=sklearn.metrics.precision_score,average='weighted'),\n",
    "        \"P_samples\": partial(calc,func=sklearn.metrics.precision_score,average='samples'),\n",
    "        \"R_micro\": partial(calc,func=sklearn.metrics.recall_score,average='micro'),\n",
    "        \"R_macro\": partial(calc,func=sklearn.metrics.recall_score,average='macro'),\n",
    "        \"R_weighted\": partial(calc,func=sklearn.metrics.recall_score,average='weighted'),\n",
    "        \"R_samples\": partial(calc,func=sklearn.metrics.recall_score,average='samples'),      \n",
    "        \"Hamming_loss\": partial(calc,func=sklearn.metrics.hamming_loss),\n",
    "        \"Exact_match_ratio\": partial(calc,func=sklearn.metrics.accuracy_score),\n",
    "        \"AUC_micro\": partial(calc,func=sklearn.metrics.roc_auc_score, average='micro'),\n",
    "        \"AUC_macro\": partial(calc,func=sklearn.metrics.roc_auc_score, average='macro'),\n",
    "        \"AUC_wighted\": partial(calc,func=sklearn.metrics.roc_auc_score, average='weighted'),\n",
    "        \"Coverage_err\": partial(calc,func=sklearn.metrics.coverage_error),\n",
    "        \"Avg_P_score_micro\": partial(calc,func=sklearn.metrics.average_precision_score, average='micro'),\n",
    "        \"Avg_P_score_macro\": partial(calc,func=sklearn.metrics.average_precision_score, average='macro')      \n",
    "    }\n",
    "\n",
    "    eval_results = {i:\"{:.2f}\".format(metrics[i](y_original, y_pred, y_pred_probab)*100) for i in metrics}\n",
    "    class_report = classification_report(y_original,y_pred) \n",
    "    return eval_results, r, p ,f, s1, s5, class_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hqSy8SO6krf1"
   },
   "source": [
    "# Running Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mxMuOGfHkrgC",
    "outputId": "cfe54732-9bc4-40a6-f6c3-d44dfb77638e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#change feature mode here: 0 for tf-df, 1 for doc2vec\n",
    "feat = 0\n",
    "# change tuningmode here: default, randomized, gridsearch\n",
    "tune = 0\n",
    "#change model name here: nb, gnb, sgd, lr\n",
    "algo = 'gnb'\n",
    "algorithmName, model, y_pred_probab = classify_multilabel(algo, feat, tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T68EUDhCkrgH",
    "outputId": "09babf45-6fad-4f58-8434-e09f0aefa42d"
   },
   "outputs": [],
   "source": [
    "thr = [0.25, 0.5]\n",
    "for t in thr:    \n",
    "    print('\\n------Threshold------', t)\n",
    "    preds = np.where(y_pred_probab > t, 1, 0)\n",
    "    results, r, p, f, s1, s5, class_report = eval(y_test, preds, y_pred_probab)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "hqSy8SO6krf1",
    "UXiH8_VakrgO",
    "C8LbXhUYkrgb"
   ],
   "name": "trad_balanced_scikit2 (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
