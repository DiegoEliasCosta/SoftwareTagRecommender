{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install simpletransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from simpletransformers.classification import MultiLabelClassificationModel\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import datetime\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import sklearn \n",
    "from scipy import stats\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set the columns of dataset\n",
    "topics_col = 'github_topics_top'\n",
    "text_col = 'input_text_freq'\n",
    "\n",
    "#randomly split data, then load data\n",
    "train_df = pd.read_csv('../data/repos_multihot_train.csv')\n",
    "test_df = pd.read_csv('../data/repos_multihot_test.csv')\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to prevent from getting \"too many dimension str\" error\n",
    "#after saving a csv, lists turn to strings which can not be fed to the model below\n",
    "train_df[\"labels\"] = train_df[\"labels\"].str.strip('\\n')\n",
    "train_df[\"labels\"] = train_df[\"labels\"].str.strip('][')\n",
    "train_df[\"labels\"] = train_df[\"labels\"].str.split(' ')\n",
    "train_df[\"labels\"] = train_df[\"labels\"].apply(lambda x: list(map(int, x)))\n",
    "\n",
    "test_df[\"labels\"] = test_df[\"labels\"].str.strip('\\n')\n",
    "test_df[\"labels\"] = test_df[\"labels\"].str.strip('][')\n",
    "test_df[\"labels\"] = test_df[\"labels\"].str.split(' ')\n",
    "test_df[\"labels\"] = test_df[\"labels\"].apply(lambda x: list(map(int, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate weights based on frequency of labels for balancing the data\n",
    "cols = train_df.columns.difference(['text','labels'])\n",
    "freq = np.sum(train_df[cols], axis = 0)\n",
    "w = max(freq) / freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_name = 'outputs/github220t_epoch6_disitillbert'\n",
    "num_selected_labels = 220\n",
    "#change the model names and paths here: bert, distilbert, albert, roberta, xlm, xlnet\n",
    "model = MultiLabelClassificationModel('distilbert', 'distilbert-base-uncased',\n",
    "                                      num_labels = num_selected_labels, \n",
    "                                      use_cuda = True, \n",
    "                                      cuda_device = 0,\n",
    "                                      pos_weight = list(w),\n",
    "                                      args={'gradient_accumulation_steps':8,\n",
    "                                            'learning_rate': 3e-5, \n",
    "                                            'num_train_epochs': 6,\n",
    "                                            'max_seq_length': 512,\n",
    "                                            'train_batch_size':4,                                             \n",
    "                                            'overwrite_output_dir': True,\n",
    "                                            'output_dir': output_name +'/',\n",
    "                                            \"n_gpu\": 1,\n",
    "                                            'reprocess_input_data': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "start_training = datetime.datetime.strftime(datetime.datetime.today(), '%d/%m/%Y-%H:%M')\n",
    "model.train_model(train_df)\n",
    "end_training = datetime.datetime.strftime(datetime.datetime.today(), '%d/%m/%Y-%H:%M')\n",
    "print(start_training, end_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation\n",
    "def calc(p1, p2, func, **kwargs):\n",
    "    p2 = [list(map(lambda x: 1 if x > 0.5 else 0,y)) for y in p2]\n",
    "    return func(p1, p2, **kwargs)\n",
    "\n",
    "def calc_recom(p1, p2, func, **kwargs):\n",
    "    return func(p1, p2, **kwargs)\n",
    "\n",
    "def success_rate(y_original, y_pred):\n",
    "    common = 0\n",
    "    for i in range(0, len(y_pred)):\n",
    "        if(sum(y_original[i] * y_pred[i])) > 0:\n",
    "            common = common +1\n",
    "    success = common/len(y_pred)\n",
    "    return success\n",
    "\n",
    "def coverage(y_original,y_pred):\n",
    "    x =  np.sum(y_pred, axis = 0)\n",
    "    c = np.count_nonzero(x > 0)\n",
    "    cov = c / num_selected_labels\n",
    "    return cov    \n",
    "\n",
    "def prf_at_k(y_original, y_pred_probab):\n",
    "    org_label_count_vec = np.sum(y_original, axis=1)\n",
    "    repo_5_tags = len(np.where(org_label_count_vec >= 5)[0])\n",
    "    k_list = [1, 2, 3, 5, 8, 10]\n",
    "    s1, s5 = {}, {}\n",
    "    r, p,f =  {}, {}, {}\n",
    "\n",
    "    for k in k_list:\n",
    "        org_label_count = np.sum(y_original, axis=1).tolist()\n",
    "        top_ind = []\n",
    "        top_ind =  np.argpartition(y_pred_probab, -1 * k, axis=1)[:, -1 * k:]\n",
    "        pred_in_org = y_original[np.arange(len(y_original))[:, None], top_ind]\n",
    "        common_topk = np.sum(pred_in_org, axis=1)\n",
    "        recall, precision, f1 = [], [], []\n",
    "        success1, success5 = 0, 0\n",
    "        for index, value in enumerate(common_topk):    \n",
    "            recall.append(value/min(k, org_label_count[index]))\n",
    "            precision.append(value/k)          \n",
    "            if (value >= 1): success1 += 1          \n",
    "            if (value >= 5): success5 += 1                 \n",
    "        s1.update({'S1@'+str(k): \"{:.2f}\".format((success1/len(y_original))*100)})\n",
    "        s5.update({'S5@'+str(k): \"{:.2f}\".format((success5/repo_5_tags)*100)})\n",
    "        r.update({'R@'+str(k): \"{:.2f}\".format(np.mean(recall)*100)})           \n",
    "        p.update({'P@'+str(k): \"{:.2f}\".format(np.mean(precision)*100)})\n",
    "        f1 = stats.hmean([precision, recall])\n",
    "        f.update({'F1@'+str(k): \"{:.2f}\".format(np.mean(f1)*100)})\n",
    "    return r, p, f, s1, s5\n",
    "\n",
    "metrics_recom = {\n",
    "    \"Success_Rate\": partial(calc,func=success_rate),\n",
    "    \"Coverage\": partial(calc,func=coverage),\n",
    "    \"LRL\": partial(calc,func=sklearn.metrics.label_ranking_loss),\n",
    "    \"AUC_micro\": partial(calc,func=sklearn.metrics.roc_auc_score, average='micro'),\n",
    "    \"AUC_macro\": partial(calc,func=sklearn.metrics.roc_auc_score, average='macro'),\n",
    "    \"AUC_wighted\": partial(calc,func=sklearn.metrics.roc_auc_score, average='weighted'),\n",
    "    \"Coverage_err\": partial(calc,func=sklearn.metrics.coverage_error),\n",
    "    \"Avg_P_score_micro\": partial(calc,func=sklearn.metrics.average_precision_score, average='micro'),\n",
    "    \"Avg_P_score_macro\": partial(calc,func=sklearn.metrics.average_precision_score, average='macro'),     \n",
    "    \"R@k\": partial(calc_recom,func=prf_at_k),\n",
    "    \"f1_micro\": partial(calc,func=sklearn.metrics.f1_score,average='micro'),\n",
    "    \"f1_macro\": partial(calc,func=sklearn.metrics.f1_score,average='macro'),\n",
    "    \"f1_weighted\": partial(calc,func=sklearn.metrics.f1_score,average='weighted'),\n",
    "    \"f1_samples\": partial(calc,func=sklearn.metrics.f1_score,average='samples'),\n",
    "    \"prec_micro\": partial(calc,func=sklearn.metrics.precision_score,average='micro'),\n",
    "    \"prec_macro\": partial(calc,func=sklearn.metrics.precision_score,average='macro'),\n",
    "    \"prec_weighted\": partial(calc,func=sklearn.metrics.precision_score,average='weighted'),\n",
    "    \"prec_samples\": partial(calc,func=sklearn.metrics.precision_score,average='samples'),\n",
    "    \"recall_micro\": partial(calc,func=sklearn.metrics.recall_score,average='micro'),\n",
    "    \"recall_macro\": partial(calc,func=sklearn.metrics.recall_score,average='macro'),\n",
    "    \"recall_weighted\": partial(calc,func=sklearn.metrics.recall_score,average='weighted'),\n",
    "    \"recall_samples\": partial(calc,func=sklearn.metrics.recall_score,average='samples'),\n",
    "    \"hamming_loss\": partial(calc,func=sklearn.metrics.hamming_loss),\n",
    "    \"exact_match_ratio\": partial(calc,func=sklearn.metrics.accuracy_score)   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eval_results, model_outputs, wrong_predictions = model.eval_model(test_df, verbose=True, **metrics_recom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_name + '/fullreport.txt','w') as f:\n",
    "        f.write(str(eval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction for sample inputs\n",
    "predictions, raw_outputs = model.predict(['python python test java javascript meditation'])\n",
    "print(predictions)\n",
    "print(raw_outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
